# -*- coding: utf-8 -*-
"""Result.ipynb

Automatically generated by Colab.

# Script finale per il tirocinio

# Parte necessaria per Colab

## Carico i modelli
"""

from google.colab import files
files.upload()

"""## Carico il file .txt con l'input"""

files.upload()

"""# Carico i dati necessari

## Carico i modelli
"""

import keras

dense_model = keras.saving.load_model("dense.keras")
recurrent_model = keras.saving.load_model("recurrent.keras")
convolutional_model = keras.saving.load_model("convolutional.keras")

"""## Carico i dati di input"""

text_input = []
f = open("input.txt", "r")
for x in f:
  text_input.append(x)
f.close()

"""# Setup LIME

## Funzioni utili
"""

from keras.datasets import imdb

# Load the IMDb dataset's word index (mapping from words to integer indices)
word_index = imdb.get_word_index()

# Indices 1 and 2 are reserved for special tokens
# 1 -> "start of sequence", 2 -> "padding"
INDEX_FROM = 3  # We will offset by 3 to account for these reserved tokens

# Invert the word index to map integer indices back to words
reverse_word_index = {value + INDEX_FROM: key for (key, value) in word_index.items()}

# Function to decode a sequence back to text
def decode_sequence(sequence):
    # Indices start from 3 (1 and 2 are reserved)
    return ' '.join([reverse_word_index.get(i, '?') for i in sequence])

# Special token for out-of-vocabulary (OOV) words
OOV_TOKEN = 3  # Maps to index 3

# Function to encode a list of texts into sequences of integers
def encode_sequences(texts):
    encoded_sequences = []

    # Iterate over each text in the list
    for text in texts:
        # Ensure the text is in lowercase and split into words
        words = text.lower().split()

        # Map each word to the corresponding integer index in the word_index
        # If a word is not found in the word_index, map it to the OOV token
        sequence = [word_index.get(word, OOV_TOKEN) + INDEX_FROM for word in words]

        encoded_sequences.append(sequence)

    return encoded_sequences

"""## Configurazione"""

!pip install lime -q

import numpy as np
import lime

from lime.lime_text import LimeTextExplainer
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

from sklearn.base import BaseEstimator, TransformerMixin

class CustomVectorizer(BaseEstimator, TransformerMixin):
  def __init__(self, word_index):
    # Store the word index, which maps words to integer indices
    self.word_index = word_index

  def fit(self, X, y=None):
    # No fitting needed for this vectorizer, just return self
    return self

  def transform(self, X):
    return encode_sequences(X)

model = dense_model

# Create an instance of the CustomVectorizer, passing the word_index
custom_vectorizer = CustomVectorizer(word_index=word_index)

# Create a pipeline with the custom vectorizer and your model
pipeline = make_pipeline(custom_vectorizer, model)

class_names = ['Negative', 'Positive']
# Create a LIME explainer
explainer = LimeTextExplainer(class_names=class_names)

def predict_fn(texts):
  # Tokenize the raw text using the custom vectorizer
  tokenized_texts = custom_vectorizer.transform(texts)

  # Pad all sequences to the same length
  max_sequence_length = maxlen
  padded_sequences = preprocessing.sequence.pad_sequences(tokenized_texts, maxlen=max_sequence_length)

  # Use the model to predict the class probabilities
  predictions = model.predict(padded_sequences)

  # Since it's a binary classification, convert the single output to two probabilities
  predictions = np.hstack((1 - predictions, predictions))

  return predictions

"""# Mostro le predizioni"""

import IPython
import time
from IPython.display import display, HTML
from keras import preprocessing

maxlen = 100

"""## Dense model"""

model = dense_model

i = 0
for text in text_input:
  exp = explainer.explain_instance(text, predict_fn, num_features=10, labels=[0])

  outputFileName = 'dense' + str(i) + '.html'
  exp.save_to_file(outputFileName)
  display(HTML(filename=outputFileName))
  i = i + 1

"""## Recurrent Model"""

model = recurrent_model

i = 0
for text in text_input:
  exp = explainer.explain_instance(text, predict_fn, num_features=10, labels=[0])

  outputFileName = 'recurrent' + str(i) + '.html'
  exp.save_to_file(outputFileName)
  display(HTML(filename=outputFileName))
  i = i + 1

"""## Convolutional Model"""

model = convolutional_model

i = 0
for text in text_input:
  exp = explainer.explain_instance(text, predict_fn, num_features=10, labels=[0])

  outputFileName = 'convolutional' + str(i) + '.html'
  exp.save_to_file(outputFileName)
  display(HTML(filename=outputFileName))
  i = i + 1